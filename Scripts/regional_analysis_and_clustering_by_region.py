# -*- coding: utf-8 -*-
"""Regional_analysis_and_clustering_by_region.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VGFUhEp91Dh-NKwGZYC-ytaNZcM7EGEh

#EDA on Customer_route data : Regional Capacity Utilization of Representatives, Existing Route Analysis, DBSCAN Clustering in regions
--------------------------------------------------------------------------------
**Date:** 18th Mar 2025

**Author:** N Priyanka

**Datasets used:** Customer_route.csv and Representative data from Omdena_Repzone_v1.xlsx

-------------------------------------------------------------------------------

The analysis provides a strong foundation for developing actionable recommendations to improve the efficiency and effectiveness of the sales representative network. Further steps could involve using the clustering results to design optimized territories and routes, and simulating the impact of different resource allocation strategies.

**Key ideas and themes**

1. Low Capacity Utilization: A significant overarching theme is the very low capacity utilization of the representative workforce across all regions. This suggests potential for optimization by rebalancing workloads or potentially reducing the number of representatives in certain areas.

2. Geographical Insights: The geographical analysis provides valuable information about customer distribution by region and city, as well as the actual distances traveled by representatives. This can inform decisions about route planning and territory alignment.

3. Customer Clustering: The identification of natural customer clusters within regions offers a data-driven approach to optimizing routes. Routes that align well with these clusters (high primary cluster percentage) are likely more efficient in terms of travel time and resource allocation.

4. Route Inefficiencies: The analysis of route distances and the number of clusters visited can highlight potentially inefficient routes that cover large distances or span multiple distinct customer clusters on the same day.

5. Data-Driven Optimization: The notebook provides a comprehensive framework for using data to understand current sales operations and identify areas for improvement, such as capacity planning, territory design, and route optimization.

Installing the required libraries
"""

!pip install contextily --q

"""Importing the necessary libraries."""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
from shapely.geometry import Point
import contextily as ctx  # For basemaps
import warnings
warnings.filterwarnings('ignore')

"""Reading the Customer_route.csv file into a pandas dataframe and basic exploration of the data."""

df = pd.read_csv("/content/drive/MyDrive/Repzone/Customer_route.csv")

df.info()

metadata = []

for col in df.columns:
    # Extract column name and data type
    col_name = col
    col_type = df[col].dtype

    # For description, you can add a placeholder or actual descriptions if available
    col_description = ""  # Placeholder for description

    metadata.append({
        'Column Name': col_name,
        'Description': col_description,
        'Data Type': col_type
    })

# Create a DataFrame from the column information
metadata = pd.DataFrame(metadata)

metadata

metadata.to_csv("/content/drive/MyDrive/Repzone/route_metadata.csv", index=False)

df.columns

df[['RouteDayOfTheWeek\n(Mon, Tue, Wed, Thu, Fri, Sat, Sun)','RouteDay']]

df['RouteDayOfTheWeek\n(Mon, Tue, Wed, Thu, Fri, Sat, Sun)'].value_counts()

df['MustVisitDays'].value_counts()

# Count of the number of representatives
df['RouteRepresentativeId'].nunique()

# Count of the number of customers
df['CustomerId'].nunique()

"""## Merging the Representatives Data with the Customer_route data to calculate the Capacity Utilization by Region.

**Aggregate and Regional Utilization:** Overall and region-specific capacity utilization percentages are calculated by comparing the total weekly customer demand (based on visit frequency) with the total weekly representative capacity.

**Individual Representative Utilization:** Utilization percentages are also calculated for each representative based on their weekly customer count and capacity.

**Additional Metrics:** The analysis includes calculating the number of representatives and customers per region, as well as the customers per representative ratio.
"""

# Reading the Representative data
rep_df = pd.read_excel("/content/drive/MyDrive/Repzone/Omdena_Repzone_v1.xlsx", sheet_name='Representative')

def calculate_capacity_utilization(representative_df, customer_route_df):
    """
    Calculate capacity utilization of representatives by region based on visit frequency.
    Includes both individual-level and aggregate-level calculations.
    """
    # Step 1: Merge the dataframes on representative ID
    merged_df = pd.merge(
        customer_route_df,
        representative_df,
        left_on='RouteRepresentativeId',
        right_on='RepresentativeId',
        how='left'
    )

    # Step 2: Clean and process the VisitFreq column
    merged_df['VisitFreq'] = pd.to_numeric(merged_df['VisitFreq'], errors='coerce')
    merged_df['VisitFreq'] = merged_df['VisitFreq'].fillna(1)

    # Step 3: Calculate weekly workload based on visit frequency
    merged_df['WeeklyVisitWeight'] = 1.0 / merged_df['VisitFreq']

    # Step 4: Calculate working days per week for each representative
    def count_working_days(working_days_str):
        if pd.isna(working_days_str):
            return 6  # Default to 6 working days if missing
        try:
            days = str(working_days_str).split(',')
            return len(days)
        except:
            return 6  # Default to 6 working days if there's an error

    merged_df['WorkingDaysCount'] = merged_df['WorkingDays'].apply(count_working_days)

    # Step 5: Calculate weekly capacity for each representative
    merged_df['WeeklyCapacity'] = merged_df['DailyMaxCustomerCount'] * merged_df['WorkingDaysCount']

    # Step 6: Calculate aggregate utilization
    rep_df = merged_df[['RepresentativeId', 'DailyMaxCustomerCount', 'WorkingDaysCount']].drop_duplicates()
    total_weekly_rep_capacity = (rep_df['DailyMaxCustomerCount'] * rep_df['WorkingDaysCount']).sum()

    customer_df = merged_df[['CustomerId', 'VisitFreq']].drop_duplicates()
    customer_df['WeeklyVisitEquivalent'] = 1 / customer_df['VisitFreq']
    total_weekly_customer_demand = customer_df['WeeklyVisitEquivalent'].sum()

    aggregate_utilization = (total_weekly_customer_demand / total_weekly_rep_capacity * 100)

    # Step 7: Calculate region-level aggregate utilization
    region_rep_df = merged_df[['region', 'RepresentativeId', 'DailyMaxCustomerCount', 'WorkingDaysCount']].drop_duplicates()
    region_rep_capacity = region_rep_df.groupby('region').apply(
        lambda x: (x['DailyMaxCustomerCount'] * x['WorkingDaysCount']).sum()
    ).reset_index(name='RegionWeeklyCapacity')

    region_customer_df = merged_df[['region', 'CustomerId', 'VisitFreq']].drop_duplicates()
    region_customer_df['WeeklyVisitEquivalent'] = 1 / region_customer_df['VisitFreq']
    region_customer_demand = region_customer_df.groupby('region')['WeeklyVisitEquivalent'].sum().reset_index(
        name='RegionWeeklyDemand'
    )

    region_utilization = pd.merge(region_rep_capacity, region_customer_demand, on='region', how='outer')
    region_utilization['RegionUtilizationPercentage'] = (
        region_utilization['RegionWeeklyDemand'] / region_utilization['RegionWeeklyCapacity'] * 100
    )

    # Step 8: Calculate individual representative utilization
    # Group by representative and region to get weekly customer count for each rep
    rep_weekly_customers = merged_df.groupby(['RepresentativeId', 'region'])['WeeklyVisitWeight'].sum().reset_index(
        name='RepWeeklyCustomerCount'
    )

    # Get the weekly capacity for each representative
    rep_capacity = merged_df[['RepresentativeId', 'WeeklyCapacity', 'region']].drop_duplicates()

    # Merge to calculate individual utilization
    rep_utilization = pd.merge(rep_weekly_customers, rep_capacity, on=['RepresentativeId', 'region'], how='left')
    rep_utilization['RepUtilizationPercentage'] = (
        rep_utilization['RepWeeklyCustomerCount'] / rep_utilization['WeeklyCapacity'] * 100
    ).clip(upper=100)  # Cap at 100% for clarity

    # Step 9: Calculate average individual utilization by region
    avg_rep_utilization = rep_utilization.groupby('region')['RepUtilizationPercentage'].mean().reset_index(
        name='AvgIndividualUtilizationPercentage'
    )

    # Step 10: Calculate standard deviation of utilization by region to measure balance
    std_rep_utilization = rep_utilization.groupby('region')['RepUtilizationPercentage'].std().reset_index(
        name='StdDevUtilizationPercentage'
    )

    # Step 11: Combine everything into a final report
    final_report = pd.merge(region_utilization, avg_rep_utilization, on='region', how='outer')
    final_report = pd.merge(final_report, std_rep_utilization, on='region', how='outer')

    # Add additional metrics
    # Count representatives per region
    reps_per_region = merged_df.groupby('region')['RepresentativeId'].nunique().reset_index(
        name='RepresentativeCount'
    )
    final_report = pd.merge(final_report, reps_per_region, on='region', how='outer')

    # Count customers per region
    customers_per_region = merged_df.groupby('region')['CustomerId'].nunique().reset_index(
        name='CustomerCount'
    )
    final_report = pd.merge(final_report, customers_per_region, on='region', how='outer')

    # Calculate customers per representative ratio
    final_report['CustomersPerRepresentative'] = final_report['CustomerCount'] / final_report['RepresentativeCount']
    final_report['EffectiveWeeklyCustomersPerRep'] = final_report['RegionWeeklyDemand'] / final_report['RepresentativeCount']

    return {
        'final_report': final_report,
        'rep_details': rep_utilization,
        'merged_data': merged_df,
        'aggregate_utilization': aggregate_utilization,
        'total_weekly_rep_capacity': total_weekly_rep_capacity,
        'total_weekly_customer_demand': total_weekly_customer_demand
    }

def visualize_utilization(results):
    """
    Creates visualizations for the capacity utilization data.
    """
    final_report = results['final_report']
    rep_details = results['rep_details']
    aggregate_utilization = results['aggregate_utilization']
    total_weekly_rep_capacity = results['total_weekly_rep_capacity']
    total_weekly_customer_demand = results['total_weekly_customer_demand']

    # Set the style for all plots
    sns.set(style="whitegrid")

    # Create a figure with subplots
    fig = plt.figure(figsize=(18, 12))

    # 1. Regional Utilization Bar Chart
    ax1 = plt.subplot2grid((2, 2), (0, 0))
    sorted_report = final_report.sort_values('RegionUtilizationPercentage', ascending=False)
    sns.barplot(x='region', y='RegionUtilizationPercentage', data=sorted_report, ax=ax1, palette='viridis')
    ax1.set_title('Regional Capacity Utilization', fontsize=14)
    ax1.set_xlabel('Region', fontsize=12)
    ax1.set_ylabel('Utilization Percentage', fontsize=12)
    ax1.axhline(y=100, color='r', linestyle='--', label='Max Capacity')
    ax1.axhline(y=aggregate_utilization, color='g', linestyle='--', label=f'System Average ({aggregate_utilization:.1f}%)')
    ax1.legend()
    ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')

    # 2. Comparison between aggregate and individual utilization
    ax2 = plt.subplot2grid((2, 2), (0, 1))
    comparison_data = final_report[['region', 'RegionUtilizationPercentage', 'AvgIndividualUtilizationPercentage']]
    comparison_data = comparison_data.sort_values('RegionUtilizationPercentage', ascending=False)
    comparison_data = comparison_data.melt(id_vars=['region'],
                                          value_vars=['RegionUtilizationPercentage', 'AvgIndividualUtilizationPercentage'],
                                          var_name='Calculation Method',
                                          value_name='Utilization Percentage')
    comparison_data['Calculation Method'] = comparison_data['Calculation Method'].map({
        'RegionUtilizationPercentage': 'Aggregate Method',
        'AvgIndividualUtilizationPercentage': 'Individual Method'
    })

    sns.barplot(x='region', y='Utilization Percentage', hue='Calculation Method', data=comparison_data, ax=ax2)
    ax2.set_title('Comparison of Utilization Calculation Methods', fontsize=14)
    ax2.set_xlabel('Region', fontsize=12)
    ax2.set_ylabel('Utilization Percentage', fontsize=12)
    ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')

    # 3. Box plot showing distribution of representative utilization within each region
    ax3 = plt.subplot2grid((2, 2), (1, 0))
    rep_util_sorted = rep_details.sort_values('RepUtilizationPercentage', ascending=False)
    sns.boxplot(x='region', y='RepUtilizationPercentage', data=rep_util_sorted, ax=ax3)
    sns.swarmplot(x='region', y='RepUtilizationPercentage', data=rep_util_sorted,
                  color='black', alpha=0.5, ax=ax3)
    ax3.set_title('Distribution of Representative Utilization by Region', fontsize=14)
    ax3.set_xlabel('Region', fontsize=12)
    ax3.set_ylabel('Representative Utilization Percentage', fontsize=12)
    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, ha='right')

    # 4. Scatter plot showing relationship between capacity and demand
    ax4 = plt.subplot2grid((2, 2), (1, 1))
    sns.scatterplot(x='WeeklyCapacity', y='RepWeeklyCustomerCount', hue='region',
                   size='RepUtilizationPercentage', data=rep_details, ax=ax4)

    # Add diagonal line representing 100% utilization
    max_val = max(rep_details['WeeklyCapacity'].max(), rep_details['RepWeeklyCustomerCount'].max())
    ax4.plot([0, max_val], [0, max_val], 'r--', label='100% Utilization')

    ax4.set_title('Representative Capacity vs. Customer Demand', fontsize=14)
    ax4.set_xlabel('Weekly Capacity (Max Customers)', fontsize=12)
    ax4.set_ylabel('Weekly Customer Demand', fontsize=12)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title="Region")
    plt.tight_layout()

    # Create a heatmap showing utilization percentages for each representative
    plt.figure(figsize=(12, 10))

    # Pivot the data to create a matrix of representatives by region
    pivot_data = rep_details.pivot_table(
        index='RepresentativeId',
        columns='region',
        values='RepUtilizationPercentage',
        aggfunc='mean'
    )

    # Sort by overall utilization
    pivot_data['avg'] = pivot_data.mean(axis=1)
    pivot_data = pivot_data.sort_values('avg', ascending=False)
    pivot_data = pivot_data.drop('avg', axis=1)

    # Create heatmap
    sns.heatmap(pivot_data, cmap='YlGnBu', annot=True, fmt='.1f', linewidths=.5,
                cbar_kws={'label': 'Utilization Percentage'})
    plt.title('Utilization Heatmap by Representative and Region', fontsize=16)
    plt.tight_layout()

    # Print overall system utilization (aggregate approach)
    print("=== Overall System Capacity Utilization ===")
    print(f"Estimated weekly capacity (max customers): {total_weekly_rep_capacity:.2f}")
    print(f"Estimated weekly demand (customer visits): {total_weekly_customer_demand:.2f}")
    print(f"System-wide capacity utilization: {aggregate_utilization:.2f}%")

    # Print regional utilization statistics
    print("\n=== Regional Capacity Utilization Summary ===")
    report_cols = ['region', 'RegionWeeklyCapacity', 'RegionWeeklyDemand',
                   'RegionUtilizationPercentage', 'AvgIndividualUtilizationPercentage',
                   'StdDevUtilizationPercentage', 'RepresentativeCount', 'CustomerCount']

    sorted_report = final_report[report_cols].sort_values('RegionUtilizationPercentage', ascending=False)
    print(sorted_report.to_string(index=False, float_format=lambda x: f"{x:.2f}"))

    # Return figures for display
    return plt.gcf(), fig

results = calculate_capacity_utilization(rep_df, df)

results.get('merged_data').shape

final_report = display(results.get('final_report'))

"""## Visualization for Capacity Utilization of Representatives by Region

Regional utilization summary: Shows average utilization percentage, representative count, raw customer count, and effective weekly customer count for each region.

Representative details: Shows individual representatives' utilization rates, including their weekly customer count and capacity.
"""

visualize_utilization(results)

"""**Box-plot:**

Outliers (dots above and below the whiskers) indicate representatives with exceptionally high or low utilization.
Some regions (e.g., Aegean, Marmara) appear to have higher variability in utilization, while others (e.g., Southeastern Anatolia, Black Sea) show more consistency.

**Scatter plot:**
The x-axis represents weekly capacity (maximum customers a representative can handle), while the y-axis represents weekly customer demand.
Different colors correspond to different regions, as shown in the legend. The dashed red line represents the ideal 1:1 ratio where demand matches capacity.
Most points cluster at the lower end, suggesting low capacity utilization across regions.
The size of the dots represents representative utilization percentage, with larger dots indicating higher utilization.
"""

def identify_optimization_opportunities(results):
    """
    Identify regions and representatives that need optimization.
    """
    final_report = results['final_report']
    rep_details = results['rep_details']

    # Identify overutilized regions (>90%)
    overutilized_regions = final_report[final_report['RegionUtilizationPercentage'] > 90]

    # Identify underutilized regions (<50%)
    underutilized_regions = final_report[final_report['RegionUtilizationPercentage'] < 50]

    # Identify regions with high utilization variance (imbalanced workloads)
    imbalanced_regions = final_report[final_report['StdDevUtilizationPercentage'] > 20]

    # Identify overutilized representatives (>95%)
    overutilized_reps = rep_details[rep_details['RepUtilizationPercentage'] > 95]

    # Identify underutilized representatives (<40%)
    underutilized_reps = rep_details[rep_details['RepUtilizationPercentage'] < 40]

    print("\n=== Optimization Opportunities ===")

    if len(overutilized_regions) > 0:
        print("\nOverutilized Regions (>90% capacity):")
        print(overutilized_regions[['region', 'RegionUtilizationPercentage', 'RepresentativeCount', 'CustomerCount']].to_string(index=False))
        print("\nRecommendation: Consider adding more representatives or redistributing customers in these regions.")
    else:
        print("\nNo overutilized regions identified.")

    if len(underutilized_regions) > 0:
        print("\nUnderutilized Regions (<50% capacity):")
        print(underutilized_regions[['region', 'RegionUtilizationPercentage', 'RepresentativeCount', 'CustomerCount']].to_string(index=False))
        print("\nRecommendation: Consider reducing representative count or reassigning representatives to busier regions.")
    else:
        print("\nNo underutilized regions identified.")

    if len(imbalanced_regions) > 0:
        print("\nRegions with Imbalanced Workloads (StdDev >20%):")
        print(imbalanced_regions[['region', 'StdDevUtilizationPercentage', 'RegionUtilizationPercentage']].to_string(index=False))
        print("\nRecommendation: Redistribute customers among representatives in these regions to balance workloads.")
    else:
        print("\nNo regions with significantly imbalanced workloads identified.")

    if len(overutilized_reps) > 0:
        print("\nOverutilized Representatives (>95% capacity):")
        print(overutilized_reps[['RepresentativeId', 'region', 'RepWeeklyCustomerCount', 'WeeklyCapacity', 'RepUtilizationPercentage']].to_string(index=False))
        print("\nRecommendation: Reduce workload for these representatives or increase their capacity.")
    else:
        print("\nNo overutilized representatives identified.")

    if len(underutilized_reps) > 0:
        print("\nUnderutilized Representatives (<40% capacity):")
        print(underutilized_reps[['RepresentativeId', 'region', 'RepWeeklyCustomerCount', 'WeeklyCapacity', 'RepUtilizationPercentage']].sort_values('RepUtilizationPercentage').head(10).to_string(index=False))
        print(f"\nTotal underutilized representatives: {len(underutilized_reps)}")
        print("\nRecommendation: Increase workload for these representatives or consider reassigning them.")
    else:
        print("\nNo underutilized representatives identified.")

    # Calculate potential capacity that could be freed up by optimizing
    potential_capacity_savings = 0
    if len(underutilized_reps) > 0:
        # Calculate capacity that could be freed by optimizing underutilized reps
        potential_capacity_savings = sum(underutilized_reps['WeeklyCapacity'] * (1 - underutilized_reps['RepUtilizationPercentage']/100))

    print(f"\nPotential weekly capacity that could be freed up through optimization: {potential_capacity_savings:.2f} customer visits")

    # Generate specific recommendations for workload balancing
    print("\n=== Specific Recommendations ===")

    # Identify regions that need additional capacity
    capacity_needed_regions = overutilized_regions['region'].tolist() if len(overutilized_regions) > 0 else []

    # Identify regions with excess capacity
    capacity_excess_regions = underutilized_regions['region'].tolist() if len(underutilized_regions) > 0 else []

    if len(capacity_needed_regions) > 0 and len(capacity_excess_regions) > 0:
        print("\nConsider transferring representatives from these underutilized regions:")
        for region in capacity_excess_regions:
            print(f"- {region}")

        print("\nTo these overutilized regions:")
        for region in capacity_needed_regions:
            print(f"- {region}")

    # Suggest workload redistribution within imbalanced regions
    if len(imbalanced_regions) > 0:
        print("\nWorkload redistribution recommendations:")
        for region in imbalanced_regions['region'].tolist():
            region_reps = rep_details[rep_details['region'] == region]
            overworked_reps = region_reps[region_reps['RepUtilizationPercentage'] > 90]['RepresentativeId'].tolist()
            underworked_reps = region_reps[region_reps['RepUtilizationPercentage'] < 60]['RepresentativeId'].tolist()

            if len(overworked_reps) > 0 and len(underworked_reps) > 0:
                print(f"\nIn {region}:")
                print(f"  - Redistribute customers from representatives {', '.join(map(str, overworked_reps))} (high utilization)")
                print(f"  - To representatives {', '.join(map(str, underworked_reps))} (low utilization)")

    # Return optimization opportunities for further analysis
    return {
        'overutilized_regions': overutilized_regions,
        'underutilized_regions': underutilized_regions,
        'imbalanced_regions': imbalanced_regions,
        'overutilized_reps': overutilized_reps,
        'underutilized_reps': underutilized_reps,
        'potential_capacity_savings': potential_capacity_savings
    }

identify_optimization_opportunities(results)

"""## Converting the dataframe to a geodataframe.
This will help to calculate accurate distances between locations.
"""

! pip install kneed --q

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from shapely.geometry import Point, LineString
import geopandas as gpd
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from kneed import KneeLocator
import warnings
warnings.filterwarnings('ignore')

# Set plot style
plt.style.use('ggplot')
sns.set(font_scale=1.2)

# Load the dataset from CSV
def load_data(file_path):
    """Load the dataset from a CSV file."""
    df = pd.read_csv(file_path)
    # Clean column names by removing newlines and extra spaces
    df.columns = [col.split('\n')[0].strip() for col in df.columns]
    return df

df = load_data('/content/drive/MyDrive/Repzone/Customer_route.csv')

df.columns

# Convert DataFrame to GeoDataFrame using latitude and longitude
def create_geodataframe(df):
    """Convert a pandas DataFrame with lat/long to a GeoDataFrame."""
    # Make a copy to avoid modifying the original
    gdf = df.copy()

    # Drop rows with missing coordinates
    gdf = gdf.dropna(subset=['Latitude', 'Longitude'])

    # Create geometry column from latitude and longitude
    geometry = [Point(xy) for xy in zip(gdf['Longitude'], gdf['Latitude'])]
    gdf = gpd.GeoDataFrame(gdf, geometry=geometry)

    # Set coordinate reference system (CRS) to WGS84
    gdf.crs = "EPSG:4326"

    # Create a projected version for accurate distance calculations
    gdf_projected = gdf.to_crs(epsg=5259)

    return gdf, gdf_projected

# Create GeoDataFrame
gdf, gdf_projected = create_geodataframe(df)
print(f"GeoDataFrame created with {len(gdf)} valid coordinates")
display(gdf.head())

gdf.info()

# Checking whether there are invalid geometries in the data.
np.where(gdf.is_valid==False)

# Convert time strings to datetime objects for easier manipulation
def time_str_to_minutes(time_str):
    """Convert time string (HH:MM) to minutes since midnight"""
    hours, minutes = map(int, time_str.split(':'))
    return hours * 60 + minutes

# Process customer time windows
gdf['EligibilityBeginMinutes'] = gdf['EligibilityBeginTime (hh:mm)'].apply(time_str_to_minutes)
gdf['EligibilityEndMinutes'] = gdf['EligibilityEndTime (hh:mm)'].apply(time_str_to_minutes)
gdf['VisitDuration'] = gdf['VisitDuration'].apply(time_str_to_minutes)

gdf['VisitDuration'].dtype

gdf.info()

# Analyze average visit frequency and duration by region and city
def analyze_regional_metrics(gdf):
    """Analyze and visualize the average visit frequency and duration by region and city."""
    # Region-based analysis
    region_metrics = gdf.groupby('region').agg({
        'VisitFreq': 'mean',
        'VisitDuration': 'mean',
        'CustomerId': 'count'
    }).reset_index().sort_values(by='CustomerId', ascending=False)

    # Rename columns in region_metrics
    region_metrics = region_metrics.rename(columns={
        'VisitFreq': 'Average Visit Frequency',
        'VisitDuration': 'Average Visit Duration',
        'CustomerId': 'Number of Customers'
    })

    # City-based analysis
    city_metrics = gdf.groupby('City').agg({
        'VisitFreq': 'mean',
        'VisitDuration': 'mean',
        'CustomerId': 'count'
    }).reset_index().sort_values(by='CustomerId', ascending=False).head(15)

    # Rename columns in region_metrics
    city_metrics = city_metrics.rename(columns={
        'VisitFreq': 'Average Visit Frequency',
        'VisitDuration': 'Average Visit Duration',
        'CustomerId': 'Number of Customers'
    })


    # Display metrics
    print("Regional Metrics:")
    display(region_metrics)

    print("\nTop 15 Cities by Customer Count:")
    display(city_metrics)

    # Visualize region metrics
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12))

    sns.barplot(x='region', y='Average Visit Frequency', data=region_metrics, ax=ax1)
    ax1.set_title('Average Visit Frequency by Region')
    ax1.set_xlabel('Region')
    ax1.set_ylabel('Average Visit Frequency')
    ax1.tick_params(axis='x', rotation=45)

    sns.barplot(x='region', y='Average Visit Duration', data=region_metrics, ax=ax2)
    ax2.set_title('Average Visit Duration by Region')
    ax2.set_xlabel('Region')
    ax2.set_ylabel('Average Visit Duration (minutes)')
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

    # Visualize city metrics
    plt.figure(figsize=(14, 8))
    city_plot = sns.barplot(x='City', y='Number of Customers', data=city_metrics)
    plt.title('Customer Count by Top 15 Cities')
    plt.xlabel('City')
    plt.ylabel('Number of Customers')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

    return region_metrics, city_metrics

# Execute regional analysis
region_metrics, city_metrics = analyze_regional_metrics(gdf)

"""Plotting the existing routes of Representatives by route day."""

def analyze_representative_routes_geo(gdf):
    """Analyze routes using spatial data techniques."""
    # Select a sample of representatives
    sample_reps = gdf['RouteRepresentativeId'].value_counts().head(5).index.tolist()

    for rep_id in sample_reps:
        rep_routes = gdf[gdf['RouteRepresentativeId'] == rep_id]

        for day in rep_routes['RouteDay'].unique():
            day_routes = rep_routes[rep_routes['RouteDay'] == day].sort_values('RouteVisitOrder')

            if len(day_routes) > 1:
                # Create a map with day's route
                fig, ax = plt.subplots(figsize=(12, 10))
                day_routes.plot(ax=ax, color='blue', markersize=50)

                # Create a line connecting points in visit order
                if len(day_routes) >= 2:
                    # Create LineString from points in order
                    route_line = day_routes.geometry.tolist()
                    route_gdf = gpd.GeoDataFrame(geometry=[LineString(route_line)], crs=day_routes.crs)
                    route_gdf.plot(ax=ax, color='red', linewidth=2)

                # Add a basemap
                ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)

                # Add labels
                for idx, row in day_routes.iterrows():
                    plt.annotate(f"{row['RouteVisitOrder']}",
                                xy=(row.geometry.x, row.geometry.y),
                                xytext=(3, 3), textcoords="offset points")

                plt.title(f"Representative {rep_id} - Day {day} Route")
                # plt.savefig(f"geo_rep_{rep_id}_day_{day}_route.png", dpi=300, bbox_inches='tight')
                # plt.close()
                plt.show()

analyze_representative_routes_geo(gdf)

"""Distance calculation using the geodataframe for accurate distance measurements.

Considering the CRS of Turkey as EPSG: 5259
"""

# More accurate distance calculation using projected coordinates
def calculate_route_distances(gdf):
    """Calculate actual distances between consecutive stops."""
    # Ensure we're working with a projected CRS for accurate measurements
    if gdf.crs.is_geographic:
        gdf_proj = gdf.to_crs(epsg=5259)
    else:
        gdf_proj = gdf.copy()

    results = []

    for (rep_id, day), group in gdf_proj.groupby(['RouteRepresentativeId', 'RouteDay']):
        if len(group) < 2:
            continue

        # Sort by visit order
        group = group.sort_values('RouteVisitOrder')

        # Calculate distances between consecutive points
        total_distance = 0
        for i in range(len(group) - 1):
            point1 = group.iloc[i].geometry
            point2 = group.iloc[i+1].geometry
            # Distance in meters (since we're using a projected CRS)
            distance = point1.distance(point2)
            total_distance += distance

        # Convert to kilometers
        total_distance_km = total_distance / 1000

        results.append({
            'RouteRepresentativeId': rep_id,
            'RouteDay': day,
            'TotalVisits': len(group),
            'TotalDistanceKm': total_distance_km,
            'AvgDistanceBetweenVisits': total_distance_km / (len(group) - 1) if len(group) > 1 else 0
        })

    return pd.DataFrame(results)

route_distances = calculate_route_distances(gdf)
display(route_distances)

import matplotlib.dates as mdates


# Assuming route_distances is your DataFrame
# Convert RouteDay to datetime if it's not already
if not pd.api.types.is_datetime64_any_dtype(route_distances['RouteDay']):
    route_distances['RouteDay'] = pd.to_datetime(route_distances['RouteDay'])

# Set the style for a more professional look
sns.set_style("whitegrid")

# Create the figure and axis objects
fig, ax = plt.subplots(figsize=(14, 7))

# Create bar chart
bars = ax.bar(route_distances['RouteDay'], route_distances['TotalDistanceKm'],
        width=0.8, color='#1f77b4', alpha=0.7)

# Format the x-axis to show dates properly
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
ax.xaxis.set_major_locator(mdates.DayLocator(interval=3))  # Show every 3rd day
plt.xticks(rotation=45, ha='right')  # Rotate date labels

# Add labels and title with better formatting
plt.xlabel('Route Day', fontsize=12)
plt.ylabel('Total Distance (km)', fontsize=12)
plt.title('Daily Route Distance Over Time', fontsize=16, fontweight='bold')

# Highlight the maximum distance
max_dist_idx = route_distances['TotalDistanceKm'].idxmax()
max_date = route_distances.loc[max_dist_idx, 'RouteDay']
max_dist = route_distances.loc[max_dist_idx, 'TotalDistanceKm']

# Highlight the maximum bar
max_bar = bars[max_dist_idx]
max_bar.set_color('red')
max_bar.set_alpha(1.0)

# Add annotation for maximum
ax.annotate(f'Max: {max_dist:.1f} km',
            xy=(max_date, max_dist),
            xytext=(0, 10),
            textcoords='offset points',
            ha='center',
            fontweight='bold')

# Show mean distance as a horizontal line
mean_dist = route_distances['TotalDistanceKm'].mean()
ax.axhline(y=mean_dist, color='darkred', linestyle='--', alpha=0.7, linewidth=2)
ax.annotate(f'Avg: {mean_dist:.1f} km',
            xy=(route_distances['RouteDay'].iloc[len(route_distances)//2], mean_dist),
            xytext=(0, 10),
            textcoords='offset points',
            ha='center',
            fontweight='bold',
            color='darkred')

# Ensure layout is tight
plt.tight_layout()

# Show the plot
plt.show()

# 3. Analyze customer distribution by district
def analyze_district_distribution(gdf):
    """Analyze and visualize the distribution of customers and visit frequency by district."""
    district_metrics = gdf.groupby('District').agg({
        'CustomerId': 'count',
        'VisitFreq': 'sum'
    }).reset_index().sort_values(by='CustomerId', ascending=False).head(20)

    # Visualize top districts
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))

    sns.barplot(x='District', y='CustomerId', data=district_metrics, ax=ax1)
    ax1.set_title('Number of Customers by Top 20 Districts')
    ax1.set_xlabel('District')
    ax1.set_ylabel('Number of Customers')
    ax1.tick_params(axis='x', rotation=90)

    sns.barplot(x='District', y='VisitFreq', data=district_metrics, ax=ax2)
    ax2.set_title('Total Visit Frequency by Top 20 Districts')
    ax2.set_xlabel('District')
    ax2.set_ylabel('Total Visit Frequency')
    ax2.tick_params(axis='x', rotation=90)

    plt.tight_layout()
    plt.show()

    return district_metrics

distance_metrics = analyze_district_distribution(gdf)

# 5. Analyze relationship between VisitFreq and other variables
def analyze_visit_frequency_relationships(gdf):
    """Analyze how VisitFreq relates to other attributes."""
    # Analyze relationship between VisitFreq and RouteRepresentativeId
    rep_freq = gdf.groupby('RouteRepresentativeId')['VisitFreq'].mean().reset_index()
    rep_freq = rep_freq.sort_values(by='VisitFreq', ascending=False)

    # Top 20 representatives by average visit frequency
    top_reps = rep_freq.head(20)

    # Analyze relationship between VisitFreq and geographic location
    geo_freq = gdf.groupby(['region', 'City'])['VisitFreq'].mean().reset_index()
    geo_freq = geo_freq.sort_values(by='VisitFreq', ascending=False)

    # Visualize top representatives
    plt.figure(figsize=(12, 6))
    sns.barplot(x='RouteRepresentativeId', y='VisitFreq', data=top_reps)
    plt.title('Average Visit Frequency by Top 20 Representatives')
    plt.xlabel('Representative ID')
    plt.ylabel('Average Visit Frequency')
    plt.xticks(rotation=90)
    plt.savefig("rep_freq_relationship.png")
    plt.close()

    # Visualize geographic relationship
    plt.figure(figsize=(12, 6))
    sns.boxplot(x='region', y='VisitFreq', data=df)
    plt.title('Visit Frequency Distribution by Region')
    plt.xlabel('Region')
    plt.ylabel('Visit Frequency')
    plt.xticks(rotation=45)
    plt.show()

    return rep_freq, geo_freq

rep_freq, geo_freq = analyze_visit_frequency_relationships(gdf)

rep_freq

geo_freq

# 6. Time-based analysis beyond seasonality
def analyze_time_patterns(gdf):
    """Analyze time-based patterns in visit schedules."""
    # Analyze visits per route day
    day_visits = gdf['RouteDay'].value_counts().reset_index()
    day_visits.columns = ['RouteDay', 'VisitCount']
    day_visits = day_visits.sort_values(by='RouteDay')

    # Analyze visit sequence patterns
    visit_sequence = gdf.groupby(['RouteRepresentativeId', 'RouteDay'])['RouteVisitOrder'].agg(['max', 'min', 'mean']).reset_index()
    visit_sequence['VisitRange'] = visit_sequence['max'] - visit_sequence['min'] + 1

    # Visualize visits per route day
    plt.figure(figsize=(10, 6))
    sns.barplot(x='RouteDay', y='VisitCount', data=day_visits)
    plt.title('Number of Visits Scheduled per Route Day')
    plt.xlabel('Route Day')
    plt.ylabel('Number of Visits')
    plt.savefig("day_visit_count.png")
    plt.close()

    # Visualize visit sequence range
    plt.figure(figsize=(10, 6))
    sns.histplot(visit_sequence['VisitRange'], bins=20)
    plt.title('Distribution of Visit Sequence Range per Rep-Day')
    plt.xlabel('Number of Visits per Day')
    plt.ylabel('Frequency')
    plt.show()

    return day_visits, visit_sequence

analyze_time_patterns(gdf)

# 7. Customer-level analysis
def analyze_customer_patterns(gdf):
    """Analyze visit patterns at the individual customer level."""
    # Analyze visit metrics for each customer
    customer_metrics = gdf.groupby('CustomerId').agg({
        'VisitFreq': 'mean',
        'VisitDuration': 'mean',
        'RouteDay': 'count'
    }).reset_index()

    customer_metrics.columns = ['CustomerId', 'AvgVisitFreq', 'AvgVisitDuration', 'TotalVisits']

    # Identify customers with unusual patterns
    q1_freq = customer_metrics['AvgVisitFreq'].quantile(0.25)
    q3_freq = customer_metrics['AvgVisitFreq'].quantile(0.75)
    iqr_freq = q3_freq - q1_freq

    unusual_customers = customer_metrics[
        (customer_metrics['AvgVisitFreq'] < q1_freq - 1.5 * iqr_freq) |
        (customer_metrics['AvgVisitFreq'] > q3_freq + 1.5 * iqr_freq)
    ]

    # Visualize customer visit patterns
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x='AvgVisitFreq', y='AvgVisitDuration', data=customer_metrics)
    plt.title('Customer Visit Patterns: Frequency vs Duration')
    plt.xlabel('Average Visit Frequency')
    plt.ylabel('Average Visit Duration (minutes)')
    plt.savefig("customer_visit_patterns.png")
    plt.close()

    # Visualize unusual customers
    plt.figure(figsize=(10, 6))
    sns.scatterplot(
        x='AvgVisitFreq',
        y='AvgVisitDuration',
        data=unusual_customers,
        color='red',
        s=100
    )
    plt.title('Customers with Unusual Visit Patterns')
    plt.xlabel('Average Visit Frequency')
    plt.ylabel('Average Visit Duration (minutes)')
    plt.axvline(x=q1_freq - 1.5 * iqr_freq, color='gray', linestyle='--')
    plt.axvline(x=q3_freq + 1.5 * iqr_freq, color='gray', linestyle='--')
    plt.show()

    return customer_metrics, unusual_customers

analyze_customer_patterns(gdf)

"""Existing Route map by Region"""

def create_route_map_by_region(gdf):
    """Create an interactive map showing routes by regions."""
    try:
        import folium
        import random

        # Convert to WGS84 if using a projected CRS
        map_gdf = gdf.to_crs(epsg=4326) if gdf.crs != "EPSG:4326" else gdf

        # Create base map centered on data
        center = [map_gdf['Latitude'].mean(), map_gdf['Longitude'].mean()]
        mymap = folium.Map(location=center, zoom_start=7)

        # Layer control list
        layers = []

        # Create a feature group for each route
        for region_name, region_gdf in map_gdf.groupby('region'):
            # Group data by RouteRepresentativeId and RouteDay
            for (rep_id, day), route_data in region_gdf.groupby(['RouteRepresentativeId', 'RouteDay']):
                # Create a feature group for this route
                route_layer = folium.FeatureGroup(name=f"Route: {region_name} - Rep {rep_id} - Day {day}", control=True, show=False)

                # Sort by RouteVisitOrder to plot the path in order
                route_data = route_data.sort_values('RouteVisitOrder')

                # Get the coordinates for the path
                route_points = [[row['Latitude'], row['Longitude']] for _, row in route_data.iterrows()]

                # Create the route path (polyline)
                folium.PolyLine(
                    route_points,
                    color="blue",
                    weight=3,
                    opacity=0.8,
                    popup=f"Rep {rep_id} - {day}"
                ).add_to(route_layer)

                # Add markers for each stop with visit order
                for idx, row in route_data.iterrows():
                    popup_text = f"""
                    <b>Customer ID:</b> {row['CustomerId']}<br>
                    <b>Visit Duration:</b> {row['VisitDuration']} min<br>
                    <b>Route Representative ID:</b> {row['RouteRepresentativeId']}<br>
                    <b>Route Day:</b> {row['RouteDay']}<br>
                    <b>Visit Order:</b> {row['RouteVisitOrder']}
                    """

                    # Add a marker for the stop
                    folium.CircleMarker(
                        location=[row['Latitude'], row['Longitude']],
                        radius=6,
                        color="red",
                        fill=True,
                        fill_opacity=0.7,
                        popup=folium.Popup(popup_text, max_width=300)
                    ).add_to(route_layer)

                # Add the route layer to the map for each route
                route_layer.add_to(mymap)

                # Add to layers list for LayerControl
                layers.append(route_layer)

        # Add LayerControl to toggle regions and routes
        folium.LayerControl().add_to(mymap)

        # Display map
        display(mymap)

        # Save map to HTML
        mymap.save("routes_by_region_map.html")
        print("Interactive map saved to routes_by_region_map.html")

        return mymap

    except ImportError:
        print("Folium library not available. Install using: pip install folium")
    except Exception as e:
        print(f"Error creating interactive map: {e}")
        raise e

# Example call to the function (assuming `gdf` is your GeoDataFrame)
try:
    route_map = create_route_map_by_region(gdf)
    print("Map created successfully. You can toggle layers to view different routes by representative and day.")
except Exception as e:
    print(f"Could not create interactive map: {e}")

"""## Clustering the customers within regions using DBSCAN algorithm."""

# Perform clustering analysis on customers within each region
def analyze_regional_clusters(gdf_projected):
    """Perform clustering analysis on customers within each region."""
    # Dictionary to store results
    region_clusters = {}
    region_metrics = {}

    # Process each region separately
    for region_name, region_data in gdf_projected.groupby('region'):
        if len(region_data) < 5:  # Skip regions with too few points
            print(f"Skipping region {region_name} - too few points ({len(region_data)})")
            continue

        print(f"Processing region: {region_name} with {len(region_data)} points")

        # Extract coordinates for this region
        coords = np.array([(geom.x, geom.y) for geom in region_data.geometry])

        # Scale coordinates to improve clustering
        coords_scaled = StandardScaler().fit_transform(coords)

        # Find optimal eps parameter using nearest neighbors
        neighbors = NearestNeighbors(n_neighbors=min(5, len(coords_scaled)-1))
        neighbors.fit(coords_scaled)
        distances, _ = neighbors.kneighbors(coords_scaled)
        distances = np.sort(distances[:, -1])  # Get the distance to the furthest neighbor

        # Use knee point as eps value or a default if can't find knee
        try:
            knee_locator = KneeLocator(range(len(distances)),
                                    distances,
                                    curve='convex',
                                    direction='increasing')
            knee_distance = distances[knee_locator.knee] if knee_locator.knee is not None else np.median(distances)
        except:
            knee_distance = np.median(distances)

        # Perform DBSCAN clustering with region-specific parameters
        db = DBSCAN(eps=knee_distance, min_samples=3).fit(coords_scaled)
        labels = db.labels_

        # Create a copy with cluster labels
        region_gdf = region_data.copy()
        region_gdf['cluster'] = labels
        region_clusters[region_name] = region_gdf

        # Calculate metrics for this region's clustering
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        noise_points = list(labels).count(-1) if -1 in labels else 0

        region_metrics[region_name] = {
            'num_customers': len(region_data),
            'num_clusters': n_clusters,
            'noise_points': noise_points,
            'noise_percentage': (noise_points / len(region_data)) * 100 if len(region_data) > 0 else 0,
            'avg_customers_per_cluster': (len(region_data) - noise_points) / n_clusters if n_clusters > 0 else 0
        }

        # Plot the clusters for this region
        fig, ax = plt.subplots(figsize=(12, 10))
        scatter = region_gdf.plot(column='cluster', categorical=True, legend=True,
                      markersize=50, cmap='viridis', ax=ax)

        plt.title(f'Customer Clusters in {region_name} Region')
        plt.xlabel('Longitude')
        plt.ylabel('Latitude')
        plt.show()

    # Create a DataFrame from region metrics for easier viewing
    metrics_df = pd.DataFrame.from_dict(region_metrics, orient='index')
    display(metrics_df)

    return region_clusters, metrics_df

# Execute regional clustering
region_clusters, cluster_metrics = analyze_regional_clusters(gdf_projected)

def analyze_cluster_route_alignment(region_clusters, gdf):
    """Analyze how well current routes align with natural customer clusters."""
    alignment_metrics = []

    for region_name, region_gdf in region_clusters.items():
        # Skip regions with no clusters
        if region_gdf['cluster'].max() < 0:
            continue

        # For each representative working in this region
        for rep_id, rep_data in region_gdf.groupby('RouteRepresentativeId'):
            # For each day this rep works
            for day, day_data in rep_data.groupby('RouteDay'):
                # Skip days with only one customer
                if len(day_data) < 2:
                    continue

                # Count number of clusters visited on this day
                day_clusters = day_data['cluster'].unique()
                clusters_visited = len(day_clusters[day_clusters >= 0])  # Count non-noise clusters

                # Calculate cluster purity (% of visits in the same cluster)
                if len(day_data) > 0:
                    # Create a frequency distribution of clusters for non-noise points
                    valid_clusters = day_data[day_data['cluster'] >= 0]['cluster']
                    if len(valid_clusters) > 0:
                        cluster_freq = valid_clusters.value_counts(normalize=True)
                        # Get the most common cluster
                        primary_cluster = cluster_freq.index[0] if len(cluster_freq) > 0 else None
                        # Calculate percentage of visits in primary cluster
                        primary_cluster_pct = cluster_freq.iloc[0] if len(cluster_freq) > 0 else 0
                    else:
                        primary_cluster = None
                        primary_cluster_pct = 0
                else:
                    primary_cluster = None
                    primary_cluster_pct = 0

                alignment_metrics.append({
                    'region': region_name,
                    'RouteRepresentativeId': rep_id,
                    'RouteDay': day,
                    'VisitCount': len(day_data),
                    'ClustersVisited': clusters_visited,
                    'PrimaryCluster': primary_cluster,
                    'PrimaryClusterPct': primary_cluster_pct * 100
                })

    # Convert to DataFrame
    if alignment_metrics:
        alignment_df = pd.DataFrame(alignment_metrics)

        # Display the results
        print("Route-Cluster Alignment Metrics:")
        display(alignment_df.head(10))

        # Visualize cluster alignment
        plt.figure(figsize=(14, 8))
        sns.boxplot(x='region', y='PrimaryClusterPct', data=alignment_df)
        plt.title('Route Alignment with Natural Customer Clusters')
        plt.xlabel('Region')
        plt.ylabel('% of Visits in Primary Cluster (Higher = Better)')
        plt.axhline(y=75, color='green', linestyle='--', label='Good Alignment')
        plt.axhline(y=50, color='orange', linestyle='--', label='Moderate Alignment')
        plt.legend()
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

        # Show the distribution of PrimaryClusterPct
        plt.figure(figsize=(10, 6))
        sns.histplot(alignment_df['PrimaryClusterPct'], bins=20, kde=True)
        plt.title('Distribution of Route-Cluster Alignment')
        plt.xlabel('% of Visits in Primary Cluster')
        plt.ylabel('Frequency')
        plt.show()

        return alignment_df
    else:
        print("No alignment metrics could be calculated - check if clustering was successful")
        return None

# Execute route-cluster alignment analysis
alignment_df = analyze_cluster_route_alignment(region_clusters, gdf)

def calculate_route_distances(gdf_projected):
    """Calculate actual distances between consecutive stops."""
    results = []

    # Group by rep and day
    for (rep_id, day), group in gdf_projected.groupby(['RouteRepresentativeId', 'RouteDay']):
        # Ensure there are at least 2 stops to calculate distance
        if len(group) < 2:
            continue

        # Sort by visit order
        group = group.sort_values('RouteVisitOrder')

        # Calculate distances between consecutive points
        total_distance = 0
        distances = []

        for i in range(len(group) - 1):
            point1 = group.iloc[i].geometry
            point2 = group.iloc[i+1].geometry
            # Distance in meters (since we're using a projected CRS)
            distance = point1.distance(point2)
            distances.append(distance)
            total_distance += distance

        # Convert to kilometers
        total_distance_km = total_distance / 1000
        avg_distance_km = total_distance_km / (len(group) - 1)

        # Get region information
        regions = group['region'].unique()
        region = regions[0] if len(regions) == 1 else "Multiple"

        # Count how many clusters this route visits (if clusters exist)
        if 'cluster' in group.columns:
            clusters_visited = group['cluster'].nunique()
        else:
            clusters_visited = None

        results.append({
            'RouteRepresentativeId': rep_id,
            'RouteDay': day,
            'region': region,
            'TotalVisits': len(group),
            'TotalDistanceKm': total_distance_km,
            'AvgDistanceBetweenVisitsKm': avg_distance_km,
            'MaxDistanceBetweenVisitsKm': max(distances) / 1000 if distances else 0,
            'ClustersVisited': clusters_visited
        })

    route_distances = pd.DataFrame(results)

    # Display the results
    print("Route Distance Analysis:")
    display(route_distances.head())

    # Visualize distance metrics
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    sns.scatterplot(x='TotalVisits', y='TotalDistanceKm',
                   hue='region', size='AvgDistanceBetweenVisitsKm',
                   data=route_distances, ax=ax1)
    ax1.set_title('Total Route Distance vs Number of Visits')
    ax1.set_xlabel('Number of Visits')
    ax1.set_ylabel('Total Distance (km)')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title="Region")

    sns.boxplot(x='region', y='AvgDistanceBetweenVisitsKm', data=route_distances, ax=ax2)
    ax2.set_title('Average Distance Between Visits by Region')
    ax2.set_xlabel('Region')
    ax2.set_ylabel('Average Distance Between Visits (km)')
    ax2.tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

    return route_distances

# Execute route distance analysis
route_distances = calculate_route_distances(gdf_projected)

# 7. Optimize Routes Based on Clusters
def recommend_route_optimizations(alignment_df, route_distances, cluster_metrics):
    """Identify routes that could benefit from optimization based on clustering."""
    # Merge relevant metrics
    if alignment_df is not None and not alignment_df.empty:
        # Join route distance metrics with alignment metrics
        optimization_df = pd.merge(
            route_distances,
            alignment_df[['region', 'RouteRepresentativeId', 'RouteDay', 'PrimaryClusterPct']],
            on=['region', 'RouteRepresentativeId', 'RouteDay'],
            how='inner'
        )

        # Find poorly aligned routes (low primary cluster percentage, high distance)
        optimization_df['OptimizationScore'] = (
            (100 - optimization_df['PrimaryClusterPct']) *
            optimization_df['AvgDistanceBetweenVisitsKm'] /
            optimization_df['TotalVisits']
        )

        # Sort by optimization potential (higher score = more potential)
        optimizations = optimization_df.sort_values('OptimizationScore', ascending=False).head(10)

        # Display routes with highest optimization potential
        print("Top 10 Routes with Highest Optimization Potential:")
        display(optimizations)

        # Visualize optimization potential
        plt.figure(figsize=(12, 8))
        sns.scatterplot(
            x='PrimaryClusterPct',
            y='AvgDistanceBetweenVisitsKm',
            size='TotalVisits',
            hue='region',
            data=optimization_df
        )
        plt.title('Route Optimization Potential')
        plt.xlabel('% of Visits in Primary Cluster (Higher = Better)')
        plt.ylabel('Average Distance Between Visits (km)')
        plt.axvline(x=70, color='red', linestyle='--')
        plt.text(65, plt.ylim()[1]*0.9, 'Poor Cluster Alignment',
                rotation=90, color='red', verticalalignment='top')
        plt.tight_layout()
        plt.show()

        # Create optimization recommendations
        recommendations = []

        for _, route in optimizations.iterrows():
            # Get the relevant cluster data for this route
            if route['region'] in region_clusters:
                region_cluster_data = region_clusters[route['region']]
                route_data = region_cluster_data[
                    (region_cluster_data['RouteRepresentativeId'] == route['RouteRepresentativeId']) &
                    (region_cluster_data['RouteDay'] == route['RouteDay'])
                ]

                # Count customers in each cluster for this route
                cluster_counts = route_data['cluster'].value_counts()

                recommendations.append({
                    'RouteRepresentativeId': route['RouteRepresentativeId'],
                    'RouteDay': route['RouteDay'],
                    'region': route['region'],
                    'CurrentAvgDistance': route['AvgDistanceBetweenVisitsKm'],
                    'CurrentClusterAlignment': route['PrimaryClusterPct'],
                    'OptimizationScore': route['OptimizationScore'],
                    'Recommendation': (
                        f"Route visits {len(cluster_counts)} clusters when it could be more focused. "
                        f"Consider reassigning customers to group by clusters and reduce travel distance."
                    )
                })

        # Display recommendations
        if recommendations:
            recommendations_df = pd.DataFrame(recommendations)
            display(recommendations_df)
            return recommendations_df
        else:
            print("No specific recommendations generated")
            return None
    else:
        print("Insufficient data for optimization recommendations")
        return None

# Generate optimization recommendations
recommendations = recommend_route_optimizations(alignment_df, route_distances, cluster_metrics)

region_metrics

cluster_metrics

# ## 8. Generate Final Report
def generate_summary_report(region_metrics, cluster_metrics, route_distances, recommendations):
    """Generate a summary report with key findings and recommendations."""
    print("========== ROUTE OPTIMIZATION ANALYSIS SUMMARY ==========")
    print(f"\nAnalysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d')}")
    print(f"Total Regions Analyzed: {len(region_metrics)}")
    print(f"Total Routes Analyzed: {len(route_distances)}")

    # Overall metrics
    print("\n----- OVERALL METRICS -----")
    print(f"Average Visit Duration: {gdf['VisitDuration'].mean():.2f} minutes")
    print(f"Average Visit Frequency: {gdf['VisitFreq'].mean():.2f} visits per month")
    print(f"Average Distance Between Visits: {route_distances['AvgDistanceBetweenVisitsKm'].mean():.2f} km")
    print(f"Average Total Route Distance: {route_distances['TotalDistanceKm'].mean():.2f} km")

    # Regional insights
    print("\n----- REGIONAL INSIGHTS -----")
    best_region = region_metrics.sort_values('Average Visit Duration', ascending=False).iloc[0]
    worst_region = region_metrics.sort_values('Average Visit Duration').iloc[0]

    print(f"Region with Longest Visit Duration: {best_region['region']} ({best_region['Average Visit Duration']:.2f} min)")
    print(f"Region with Shortest Visit Duration: {worst_region['region']} ({worst_region['Average Visit Duration']:.2f} min)")

    if not cluster_metrics.empty:
        most_clustered = cluster_metrics.sort_values('num_clusters', ascending=False).iloc[0]
        print(f"Region with Most Natural Clusters: {most_clustered.name} ({int(most_clustered['num_clusters'])} clusters)")

    # Route optimization potential
    print("\n----- ROUTE OPTIMIZATION POTENTIAL -----")
    if recommendations is not None and not recommendations.empty:
        print(f"Number of Routes with High Optimization Potential: {len(recommendations)}")
        print(f"Average Potential Distance Reduction: {recommendations['CurrentAvgDistance'].mean() * 0.3:.2f} km per visit")
        print(f"Routes Most Needing Optimization:")
        for idx, rec in recommendations.head(3).iterrows():
            print(f"  - Rep ID {rec['RouteRepresentativeId']} on {rec['RouteDay']} in {rec['region']}")
    else:
        print("No specific route optimization recommendations available")

    # Key recommendations
    print("\n----- KEY RECOMMENDATIONS -----")
    print("1. Restructure routes to better align with natural customer clusters")
    print("2. Prioritize optimization of routes with highest travel distances")
    print("3. Consider reassigning customers to representatives based on geographic proximity")
    print("4. Review visit frequency and duration in underperforming regions")
    print("5. Implement route planning software that accounts for customer clustering")

    print("\n========== END OF REPORT ==========")

# Generate summary report
generate_summary_report(region_metrics, cluster_metrics, route_distances, recommendations)

def analyze_routes_by_cluster(region_clusters):
    """Analyze how routes interact with clusters."""
    results = []

    # Process each region
    for region_name, region_gdf in region_clusters.items():
        # Get unique clusters in this region
        clusters = region_gdf['cluster'].unique()

        # For each cluster, analyze the routes passing through it
        for cluster_id in clusters:
            if cluster_id == -1:  # Skip noise points
                continue

            # Get data for this cluster
            cluster_data = region_gdf[region_gdf['cluster'] == cluster_id]

            # Count unique route representatives and days
            reps = cluster_data['RouteRepresentativeId'].unique()
            days = cluster_data['RouteDay'].unique()

            # For each rep/day combination, check how many clusters they visit
            for rep_id in reps:
                for day in days:
                    # Get data for this rep/day
                    route_data = region_gdf[
                        (region_gdf['RouteRepresentativeId'] == rep_id) &
                        (region_gdf['RouteDay'] == day)
                    ]

                    # Skip if no data
                    if len(route_data) == 0:
                        continue

                    # Count clusters visited on this route
                    route_clusters = route_data['cluster'].unique()
                    route_clusters = route_clusters[route_clusters >= 0]  # Exclude noise

                    # Calculate percentage of visits in this cluster
                    cluster_visits = len(route_data[route_data['cluster'] == cluster_id])
                    total_visits = len(route_data)
                    cluster_percentage = (cluster_visits / total_visits) * 100 if total_visits > 0 else 0

                    # Add to results
                    results.append({
                        'region': region_name,
                        'cluster': cluster_id,
                        'RouteRepresentativeId': rep_id,
                        'RouteDay': day,
                        'cluster_visits': cluster_visits,
                        'total_visits': total_visits,
                        'cluster_percentage': cluster_percentage,
                        'clusters_visited': len(route_clusters),
                        'other_clusters': ', '.join([str(c) for c in route_clusters if c != cluster_id])
                    })

    # Convert to DataFrame
    if results:
        analysis_df = pd.DataFrame(results)

        # Display summary
        print("\n===== Route-Cluster Interaction Analysis =====")

        # Average number of clusters visited per route
        avg_clusters = analysis_df['clusters_visited'].mean()
        print(f"Average clusters visited per route: {avg_clusters:.2f}")

        # Routes with highest cluster focus (highest percentage in a single cluster)
        focused_routes = analysis_df.sort_values('cluster_percentage', ascending=False).head(5)
        print("\nTop 5 Most Focused Routes (highest percentage in a single cluster):")
        display(focused_routes[['region', 'RouteRepresentativeId', 'RouteDay', 'cluster', 'cluster_percentage', 'clusters_visited']])

        # Routes with lowest cluster focus (visiting many clusters)
        unfocused_routes = analysis_df.sort_values('clusters_visited', ascending=False).head(5)
        print("\nTop 5 Least Focused Routes (visiting the most clusters):")
        display(unfocused_routes[['region', 'RouteRepresentativeId', 'RouteDay', 'clusters_visited', 'other_clusters']])

        # Visualization of cluster focus
        plt.figure(figsize=(12, 6))
        sns.histplot(analysis_df['cluster_percentage'], bins=20, kde=True)
        plt.title('Distribution of Route Focus (% of Visits in a Single Cluster)')
        plt.xlabel('Percentage of Route Visits in the Cluster')
        plt.ylabel('Frequency')
        plt.axvline(x=80, color='green', linestyle='--', label='Good Focus (>80%)')
        plt.axvline(x=50, color='orange', linestyle='--', label='Moderate Focus (>50%)')
        plt.legend()
        plt.show()

        return analysis_df
    else:
        print("No route-cluster interaction data available")
        return None

# Analyze how routes interact with clusters
route_cluster_analysis = analyze_routes_by_cluster(region_clusters)

def create_interactive_map_by_cluster(gdf, region_clusters):
    """Create an interactive map showing routes within clusters."""
    try:
        import folium
        from folium.plugins import MarkerCluster
        import random

        # Convert back to WGS84 if using projected CRS
        map_gdf = gdf.to_crs(epsg=4326) if gdf.crs != "EPSG:4326" else gdf

        # Create base map centered on data
        center = [map_gdf['Latitude'].mean(), map_gdf['Longitude'].mean()]
        mymap = folium.Map(location=center, zoom_start=7)

        # Generate a fixed color palette for clusters (to be consistent)
        def generate_color_palette(n_clusters):
            """Generate a fixed color palette for clusters."""
            random.seed(42)
            colors = {}

            for i in range(n_clusters):
                colors[i] = f"#{random.randint(0, 0xFFFFFF):06x}"

            # Black for noise points
            colors[-1] = "#000000"
            return colors

        # Process each region and its clusters
        all_clusters = set()
        for region_name, region_gdf in region_clusters.items():
            all_clusters.update(region_gdf['cluster'].unique())

        # Create color palette for all clusters
        cluster_colors = generate_color_palette(len(all_clusters))

        # Create a layer for each region
        layers = []
        for region_name, region_gdf in region_clusters.items():
            # Convert to WGS84 if needed
            region_map_gdf = region_gdf.to_crs(epsg=4326) if region_gdf.crs != "EPSG:4326" else region_gdf

            # Create a feature group for this region
            region_layer = folium.FeatureGroup(name=f"Region: {region_name}", control=True)

            # Create a marker cluster for this region
            region_markers = MarkerCluster(name=f"Customers in {region_name}").add_to(region_layer)

            # Add markers colored by cluster for this region
            for idx, row in region_map_gdf.iterrows():
                # Get color for this cluster
                color = cluster_colors.get(row['cluster'], "#FF0000")  # Default to red if not found

                # Create popup text
                popup_text = f"""
                <b>Customer ID:</b> {row['CustomerId']}<br>
                <b>Region:</b> {region_name}<br>
                <b>City:</b> {row['City']}<br>
                <b>Visit Duration:</b> {row['VisitDuration']} min<br>
                <b>Cluster:</b> {row['cluster']}<br>
                <b>Rep ID:</b> {row['RouteRepresentativeId']}<br>
                <b>Route Day:</b> {row['RouteDay']}<br>
                <b>Visit Order:</b> {row['RouteVisitOrder']}
                """

                # Add marker
                folium.CircleMarker(
                    location=[row['Latitude'], row['Longitude']],
                    radius=5,
                    popup=folium.Popup(popup_text, max_width=300),
                    color=color,
                    fill=True,
                    fill_opacity=0.7
                ).add_to(region_markers)

            # Add region layer to map
            layers.append(region_layer)

            # Create layers for each cluster in this region
            clusters = region_map_gdf['cluster'].unique()
            for cluster_id in clusters:
                cluster_name = f"Cluster {cluster_id}" if cluster_id >= 0 else "Noise points"
                cluster_layer = folium.FeatureGroup(name=f"{region_name} - {cluster_name}", control=True, show=False)

                # Filter data for this cluster
                cluster_data = region_map_gdf[region_map_gdf['cluster'] == cluster_id]

                # Add markers for this cluster
                for idx, row in cluster_data.iterrows():
                    # Get color for this cluster
                    color = cluster_colors.get(cluster_id, "#FF0000")

                    # Create popup text
                    popup_text = f"""
                    <b>Customer ID:</b> {row['CustomerId']}<br>
                    <b>Cluster:</b> {cluster_id}<br>
                    <b>Rep ID:</b> {row['RouteRepresentativeId']}<br>
                    <b>Route Day:</b> {row['RouteDay']}<br>
                    <b>Visit Order:</b> {row['RouteVisitOrder']}
                    """

                    # Add marker
                    folium.CircleMarker(
                        location=[row['Latitude'], row['Longitude']],
                        radius=5,
                        popup=folium.Popup(popup_text, max_width=300),
                        color=color,
                        fill=True,
                        fill_opacity=0.7
                    ).add_to(cluster_layer)

                # Add cluster layer to map
                layers.append(cluster_layer)

        # Add paths (routes) for each representative
        for region_name, region_gdf in region_clusters.items():
            # Convert to WGS84 if needed
            region_map_gdf = region_gdf.to_crs(epsg=4326) if region_gdf.crs != "EPSG:4326" else region_gdf

            # Group by representative and day
            for (rep_id, day), route_data in region_map_gdf.groupby(['RouteRepresentativeId', 'RouteDay']):
                # Create a feature group for the route
                route_layer = folium.FeatureGroup(name=f"Route: {region_name} - Rep {rep_id} - {day}", control=True, show=False)

                # Sort by visit order
                route_data = route_data.sort_values('RouteVisitOrder')

                # Get the coordinates for the path
                route_points = [[row['Latitude'], row['Longitude']] for _, row in route_data.iterrows()]

                # Create the route path
                folium.PolyLine(
                    route_points,
                    color="blue",
                    weight=3,
                    opacity=0.8,
                    popup=f"Rep {rep_id} - {day}"
                ).add_to(route_layer)

                # Add markers for each stop with visit order
                for idx, row in route_data.iterrows():
                    # Get color for this cluster
                    color = cluster_colors.get(row['cluster'], "#FF0000")

                    # Create a marker with the visit order as the label
                    folium.CircleMarker(
                        location=[row['Latitude'], row['Longitude']],
                        radius=8,
                        color=color,
                        fill=True,
                        fill_opacity=0.9,
                        popup=f"Stop {row['RouteVisitOrder']}: Cluster {row['cluster']}"
                    ).add_to(route_layer)

                    # Add a number label
                    folium.map.Marker(
                        [row['Latitude'], row['Longitude']],
                        icon=folium.DivIcon(
                            icon_size=(20, 20),
                            icon_anchor=(10, 10),
                            html=f'<div style="font-size: 10pt; color: white; text-align: center;">{row["RouteVisitOrder"]}</div>',
                        )
                    ).add_to(route_layer)

                # Add route layer to map
                layers.append(route_layer)

        # Add a legend for cluster colors
        legend_html = '''
        <div style="position: fixed; bottom: 50px; right: 50px; z-index: 1000; background-color: white;
        padding: 10px; border: 2px solid grey; border-radius: 5px; max-height: 300px; overflow-y: auto;">
        <p><b>Cluster Colors:</b></p>
        '''

        # Add a color entry for each cluster
        for cluster_id, color in sorted(cluster_colors.items()):
            if cluster_id == -1:
                legend_html += f'<p><span style="color:{color}; font-size: 20px;">&#9679;</span> Noise Points</p>'
            else:
                legend_html += f'<p><span style="color:{color}; font-size: 20px;">&#9679;</span> Cluster {cluster_id}</p>'

        legend_html += '</div>'

        # Add the legend as a custom HTML element
        mymap.get_root().html.add_child(folium.Element(legend_html))

        # Add layers to map
        for layer in layers:
            layer.add_to(mymap)

        # Add LayerControl at the end after all layers are added
        folium.LayerControl().add_to(mymap)

        # Display map
        display(mymap)

        # Save map to HTML
        mymap.save("routes_within_clusters_map.html")
        print("Interactive map saved to routes_within_clusters_map.html")

        return mymap

    except ImportError:
        print("Folium library not available. Install using: pip install folium")
    except Exception as e:
        print(f"Error creating interactive map: {e}")
        raise e

# Create a cluster-focused view of routes
try:
    cluster_route_map = create_interactive_map_by_cluster(gdf, region_clusters)
    print("Map created successfully. You can toggle layers to view different routes and clusters.")
except Exception as e:
    print(f"Could not create interactive map: {e}")